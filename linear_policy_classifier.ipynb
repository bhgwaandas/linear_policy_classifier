{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "linear_policy_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_OoGYtDVOrH"
      },
      "source": [
        "label = [[1, 'First Party Collection/Use'], \n",
        "              [2, 'Third Party Sharing/Collection'], \n",
        "              [3, 'User Choice/Control'], \n",
        "              [4, 'User Access, Edit and Deletion'], \n",
        "              [5, 'Data Retention'],\n",
        "              [6, 'Data Security'],\n",
        "              [7, 'Policy Change'], \n",
        "              [8, 'Do Not Track'],\n",
        "              [9, 'International and Specific Audiences'],\n",
        "              [10, 'Introductory/Generic'],\n",
        "              [11, 'Privacy contact information'],\n",
        "              [12, 'Privacy contact information']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waHxblAQVOrN"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hgtTM4vVRhh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjTNIsO45y0x",
        "outputId": "f9fa92a4-b170-483e-a03a-3ffb4f9ec8a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPjYOuqvVOrP"
      },
      "source": [
        "def readPolicyFile(fileLocation):\n",
        "    policySegments = []\n",
        "    for filename in os.listdir(fileLocation):\n",
        "        absFilename = \"{}/{}\".format(fileLocation,filename)\n",
        "        with open(absFilename) as csv_file:\n",
        "            #print absFilename\n",
        "            categoryId = 0\n",
        "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "            for row in csv_reader:\n",
        "                if row[5] == \"First Party Collection/Use\":\n",
        "                    categoryId = 1\n",
        "                elif row[5] == \"Third Party Sharing/Collection\":\n",
        "                    categoryId = 2\n",
        "                elif row[5] == \"User Choice/Control\":\n",
        "                    categoryId = 3\n",
        "                elif row[5] == \"User Access, Edit and Deletion\":\n",
        "                    categoryId = 4\n",
        "                elif row[5] == \"Data Retention\":\n",
        "                    categoryId = 5\n",
        "                elif row[5] == \"Data Security\":\n",
        "                    categoryId = 6\n",
        "                elif row[5] == \"Policy Change\":\n",
        "                    categoryId = 7\n",
        "                elif row[5] == \"Do Not Track\":\n",
        "                    categoryId = 8 \n",
        "                elif row[5] == \"International and Specific Audiences\":\n",
        "                    categoryId = 9\n",
        "                elif row[5] == \"Introductory/Generic\":\n",
        "                    categoryId = 10\n",
        "                elif row[5] == \"Privacy contact information\":\n",
        "                    categoryId = 11\n",
        "                elif row[5] == \"Practice not covered\":\n",
        "                    categoryId = 12\n",
        "                else:\n",
        "                    continue\n",
        "                    \n",
        "                policySegment = ''\n",
        "                jsonData=json.loads(row[6])\n",
        "                for (k, v) in jsonData.items():\n",
        "                    for (k, v) in v.items():\n",
        "                        if k == 'selectedText':\n",
        "                            policySegment = ''.join(v)\n",
        "                \n",
        "                policySegments.append([policySegment, categoryId, row[5]])\n",
        "            #print policySegments\n",
        "            #print policySegments\n",
        "    df = pd.DataFrame(policySegments, columns = ['text', 'label', 'label_name'])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHiQGczMVOrS"
      },
      "source": [
        "def cleanDocs(dataFrame):\n",
        "    cleanNull = dataFrame[df.text != 'null'].reset_index(drop=True)\n",
        "    stop = set(stopwords.words('english'))\n",
        "    exclude = set(string.punctuation) \n",
        "    lemma = WordNetLemmatizer()\n",
        "    clean_docs = []\n",
        "    bigram_docs = []\n",
        "    for index, entry in enumerate(cleanNull['text']):\n",
        "        stop_free = \" \".join([i for i in entry.lower().split() if i not in stop])\n",
        "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        "        digit_free = [word for word in punc_free.split() if not word.isdigit() and len(word) > 2]\n",
        "        normalized = \" \".join(lemma.lemmatize(word) for word in digit_free)\n",
        "        nouns = [word[0] for word in nltk.pos_tag(normalized.split()) if word[1] == 'NN' or word[1] == 'VB']\n",
        "        cleanNull.loc[index,'text_final'] = str(nouns)\n",
        "\n",
        "\t#bigram_transformer = phrases.Phrases(clean_docs)\n",
        "\t\n",
        "\t#for doc in bigram_transformer[clean_docs]:\n",
        "\t#\t\tbigram_docs.append(doc)\n",
        "    cleanEmpty = cleanNull[cleanNull.text_final != '[]']\n",
        "    return cleanEmpty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFEmVxGGVOrS"
      },
      "source": [
        "def loadTestDataset(fileName):\n",
        "    df = pd.read_csv(fileName)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO6BhS6yVOrT"
      },
      "source": [
        "def buildModel(Corpus):\n",
        "    Train_data, Test_data, Train_label, Test_label = train_test_split(Corpus['text_final'],Corpus['label'],test_size=0.3)\n",
        "    #Encoder = preprocessing.LabelEncoder()\n",
        "    #Train_label = Encoder.fit_transform(Train_label)\n",
        "    #Test_label = Encoder.fit_transform(Test_label)\n",
        "    Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "    Tfidf_vect.fit(Corpus['text_final'])\n",
        "    Train_data_Tfidf = Tfidf_vect.transform(Train_data)\n",
        "    Test_data_Tfidf = Tfidf_vect.transform(Test_data)\n",
        "    \n",
        "#     print(Tfidf_vect.vocabulary_)\n",
        "    \n",
        "    # fit the training dataset on the NB classifier\n",
        "#     Naive = MultinomialNB()\n",
        "#     Naive.fit(Train_data_Tfidf,Train_label)\n",
        "#     # predict the labels on validation dataset\n",
        "#     predictions_NB = Naive.predict(Test_data_Tfidf)\n",
        "#     # Use accuracy_score function to get the accuracy\n",
        "#     print(classification_report(Test_label, predictions_NB))\n",
        "#     print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_label)*100)\n",
        "    \n",
        "    # Classifier - Algorithm - SVM\n",
        "    # fit the training dataset on the classifier\n",
        "    SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "    SVM.fit(Train_data_Tfidf,Train_label)\n",
        "    # predict the labels on validation dataset\n",
        "    predictions_SVM = SVM.predict(Test_data_Tfidf)\n",
        "    #print Test_data_Tfidf\n",
        "    #print predictions_SVM\n",
        "    # Use accuracy_score function to get the accuracy\n",
        "    print(classification_report(Test_label, predictions_SVM))\n",
        "    print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_label)*100)\n",
        "    \n",
        "    \n",
        "    return SVM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCFjvSy-VOrU"
      },
      "source": [
        "def predictLable(model, corpus):\n",
        "    print (corpus)\n",
        "    Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "    Tfidf_vect.fit(corpus['text_final'])\n",
        "    webPolicy_TFidf = Tfidf_vect.transform(corpus['text_final'])\n",
        "    webPolicyPredition = model.predict(webPolicy_TFidf)\n",
        "    \n",
        "    return webPolicyPredition;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNB19oVWVOrU"
      },
      "source": [
        "def mergeData(corpus, predictedResult):\n",
        "    labels = [[1, 'First Party Collection/Use'], \n",
        "              [2, 'Third Party Sharing/Collection'], \n",
        "              [3, 'User Choice/Control'], \n",
        "              [4, 'User Access, Edit and Deletion'], \n",
        "              [5, 'Data Retention'],\n",
        "              [6, 'Data Security'],\n",
        "              [7, 'Policy Change'], \n",
        "              [8, 'Do Not Track'],\n",
        "              [9, 'International and Specific Audiences'],\n",
        "              [10, 'Introductory/Generic'],\n",
        "              [11, 'Privacy contact information'],\n",
        "              [12, 'Privacy contact information']]\n",
        "    \n",
        "    dfLabel = pd.DataFrame(labels, columns=['label', 'discription'])\n",
        "    dfPredictedResult = pd.DataFrame(predictedResult)\n",
        "    dfContact = pd.concat([corpus, dfPredictedResult], axis=1)\n",
        "    dfContact.columns = ['topic_number', 'corpus', 'label'] \n",
        "    return pd.merge(dfContact, dfLabel, on='label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMzJJxTM6QJG",
        "outputId": "902176e0-e9c2-4e86-de6d-ceb877308a6f"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n55An18Bdc4I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72q8cmj6VOrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c5f97b-bea2-4e77-de99-3832da7710b4"
      },
      "source": [
        "np.random.seed(500)\n",
        "fileLocation = '/content/drive/MyDrive/annotations'\n",
        "\n",
        "df = readPolicyFile(fileLocation)\n",
        "Corpus = cleanDocs(df)\n",
        "print (Corpus['label_name'].unique())\n",
        "Corpus.to_csv('clean_OOP-115_policy_corpus.csv', index=False)\n",
        "model = buildModel(Corpus)\n",
        "\n",
        "# webPolicyCorpus = loadTestDataset('topic_10_only_nouns_n_verb_run_1.csv')\n",
        "# gdprPolicyCorpus = loadTestDataset('topic_10_gdpr_only_nouns_n_verb_run_1.csv')\n",
        "\n",
        "# webPolicyPrediction = predictLable(model, webPolicyCorpus)\n",
        "# gdprPolicyPrediction = predictLable(model, gdprPolicyCorpus)\n",
        "\n",
        "# mergeData(webPolicyCorpus, webPolicyPrediction)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['First Party Collection/Use' 'User Choice/Control'\n",
            " 'Third Party Sharing/Collection' 'Data Security'\n",
            " 'International and Specific Audiences' 'Policy Change'\n",
            " 'User Access, Edit and Deletion' 'Data Retention' 'Do Not Track']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.80      0.91      0.85      2392\n",
            "           2       0.88      0.79      0.83      1387\n",
            "           3       0.64      0.74      0.68       442\n",
            "           4       0.69      0.41      0.52       227\n",
            "           5       0.81      0.23      0.35        97\n",
            "           6       0.92      0.72      0.81       310\n",
            "           7       0.87      0.71      0.78       172\n",
            "           8       1.00      0.93      0.96        29\n",
            "           9       0.90      0.87      0.88       293\n",
            "\n",
            "    accuracy                           0.81      5349\n",
            "   macro avg       0.83      0.70      0.74      5349\n",
            "weighted avg       0.82      0.81      0.81      5349\n",
            "\n",
            "SVM Accuracy Score ->  81.30491680687979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4qr-5kJhru8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8275745f-7a40-4b90-f9f0-98876c017479"
      },
      "source": [
        "\"\"\"\n",
        "import pandas as pd\n",
        "d={\"text\":[\"personal and other information that may be collected when you interact with the Barnes & Noble enterprise,\"]}\n",
        "corpus=pd.DataFrame(d)\n",
        "corpus.to_csv(\"out.csv\",index=False)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport pandas as pd\\nd={\"text\":[\"personal and other information that may be collected when you interact with the Barnes & Noble enterprise,\"]}\\ncorpus=pd.DataFrame(d)\\ncorpus.to_csv(\"out.csv\",index=False)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhU1rEMMVOrX",
        "outputId": "2208531d-6bb9-48fe-f1e3-c6c1865475de"
      },
      "source": [
        "corpus = loadTestDataset('/content/input.csv')\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    text\n",
            "0              to help customize your TED.com experience\n",
            "1         certain information from your Facebook profile\n",
            "2         certain information from your Facebook profile\n",
            "3      required for TED Conference registration or ot...\n",
            "4      required for TED Conference registration or ot...\n",
            "...                                                  ...\n",
            "17822                    we will notify parents by email\n",
            "17823  Personally Identifiable Information on communi...\n",
            "17824  Social Media Features are either hosted by a t...\n",
            "17825  Social Media Features are either hosted by a t...\n",
            "17826                send a gift certificate or playlist\n",
            "\n",
            "[17827 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjDNQO-Ngx9N",
        "outputId": "c5eaff3b-a028-4fff-e964-78bacabf0719"
      },
      "source": [
        "k= predictLable(model, cleanDocs(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                    text                                         text_final\n",
            "0              to help customize your TED.com experience                ['help', 'customize', 'experience']\n",
            "1         certain information from your Facebook profile             ['information', 'facebook', 'profile']\n",
            "2         certain information from your Facebook profile             ['information', 'facebook', 'profile']\n",
            "3      required for TED Conference registration or ot...          ['conference', 'registration', 'service']\n",
            "4      required for TED Conference registration or ot...          ['conference', 'registration', 'service']\n",
            "...                                                  ...                                                ...\n",
            "17801                    we will notify parents by email                                ['parent', 'email']\n",
            "17802  Personally Identifiable Information on communi...  ['information', 'community', 'message', 'board...\n",
            "17803  Social Media Features are either hosted by a t...                     ['medium', 'feature', 'party']\n",
            "17804  Social Media Features are either hosted by a t...                     ['medium', 'feature', 'party']\n",
            "17805                send a gift certificate or playlist        ['send', 'gift', 'certificate', 'playlist']\n",
            "\n",
            "[17806 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwnqq_I0pazb"
      },
      "source": [
        "l=list(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfUKCBaTWalI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d1f3929-52aa-4a72-c70c-c6acecb2a3ac"
      },
      "source": [
        "freq = {}\n",
        "\n",
        "for item in l:\n",
        "  if item in freq:\n",
        "    freq[item] += 1\n",
        "  \n",
        "  else:\n",
        "    freq[item] = 1\n",
        "\n",
        "print(freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 8944, 3: 1718, 2: 4275, 6: 873, 9: 910, 7: 460, 4: 442, 5: 103, 8: 81}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU178Kj0D-7L",
        "outputId": "a3e25339-b322-4b92-d4e6-d346f488c60d"
      },
      "source": [
        "list = []\n",
        "for i in range(1, 13):\n",
        "  if i == 3:\n",
        "    pass\n",
        "  elif i == 4:\n",
        "    pass\n",
        "  else:\n",
        "    try:\n",
        "      print(freq[i])\n",
        "      list.append(freq[i])\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "list"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8944\n",
            "4275\n",
            "103\n",
            "873\n",
            "460\n",
            "81\n",
            "910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8944, 4275, 103, 873, 460, 81, 910]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaTfKZl0HAp0",
        "outputId": "c570fead-6b98-446b-ccd4-01869904e970"
      },
      "source": [
        "list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8944, 4275, 103, 873, 460, 81, 910]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j2fSNuJerqD",
        "outputId": "d3619dd0-7ec1-47fb-9deb-e50f038c21d0"
      },
      "source": [
        "val = freq[3] + freq[4]\n",
        "if freq[1] > freq[2]:\n",
        "  if freq [6] > freq[5]:\n",
        "    if any(y>val for y in list ):\n",
        "      print(\"A\")\n",
        "    elif any(y<=val for y in list ):\n",
        "      print(\"B\")\n",
        "elif freq[1] <=freq[2]:\n",
        "  if freq [6] <= freq[5]:\n",
        "    if any(y>=val for y in list ):\n",
        "      print(\"C\")\n",
        "    elif any(y<=val for y in list ):\n",
        "      print(\"D\")\n",
        "  elif freq[6] > freq[5]:\n",
        "    if any(y>=val for y in list ):\n",
        "      print(\"D\")\n",
        "    if any(y<=val for y in list ):\n",
        "      print(\"D\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kNmpXeoppaD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}